{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d63bdc8-66d7-4341-aa4d-e737628d924a",
   "metadata": {},
   "source": [
    "# BERT - PRETRAINING WITH HUGGING FACE TRANSFORMERS\n",
    "# Name: Dharmraj Patil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2cd4250-684f-4796-9115-ee25aba9b6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0518276-e9a3-478b-ab78-ee3c7f818e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\patil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "581a7019-e185-4289-80c1-f43b48835c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_BATCH_SIZE = 256  # Batch-size to train the tokenizer on\n",
    "TOKENIZER_VOCABULARY = 25000  # Total number of unique subwords the tokenizer can have\n",
    "\n",
    "BLOCK_SIZE = 128  # Maximum number of tokens in an input sample\n",
    "NSP_PROB = 0.50  # Probability that the next sentence is the actual next sentence in NSP\n",
    "SHORT_SEQ_PROB = 0.1  # Probability of generating shorter sequences to minimize the mismatch between pretraining and fine-tuning.\n",
    "MAX_LENGTH = 512  # Maximum number of tokens in an input sample after padding\n",
    "\n",
    "MLM_PROB = 0.2  # Probability with which tokens are masked in MLM\n",
    "\n",
    "TRAIN_BATCH_SIZE = 2  # Batch-size for pretraining the model on\n",
    "MAX_EPOCHS = 1  # Maximum number of epochs to train the model for\n",
    "LEARNING_RATE = 1e-4  # Learning rate for training the model\n",
    "\n",
    "MODEL_CHECKPOINT = \"bert-base-cased\"  # Name of pretrained model from ðŸ¤— Model Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59935b9d-0252-4ceb-ab0c-dd4298bfe1a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b32856dc-e749-420b-9e76-84fd68f1fc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4358\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 36718\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42262884-09ab-4ccc-a6f3-af4cc48a0a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = [\n",
    "    doc for doc in dataset[\"train\"][\"text\"] if len(doc) > 0 and not doc.startswith(\" =\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abba9a89-aea8-440a-a066-5725eaeab638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator():\n",
    "    for i in range(0, len(all_texts), TOKENIZER_BATCH_SIZE):\n",
    "        yield all_texts[i : i + TOKENIZER_BATCH_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6affde70-cada-4f7f-a813-1e4a718213bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e39db091-858b-4c18-8fbc-c73b69008030",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizer.train_new_from_iterator(\n",
    "    batch_iterator(), vocab_size=TOKENIZER_VOCABULARY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1261fdfe-4503-4d9e-9301-1a814d21ffe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"].select([i for i in range(1000)])\n",
    "dataset[\"validation\"] = dataset[\"validation\"].select([i for i in range(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "338b3197-bb1e-414c-8cc3-aa4d6025ff16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a949f8112e16405eb7fded44219bddcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced543bc68f7441f9afed554f2af0d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f477c2f929d43928e65032c23eb243b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_num_tokens = BLOCK_SIZE - tokenizer.num_special_tokens_to_add(pair=True)\n",
    "def prepare_train_features(examples):\n",
    "    # Remove un-wanted samples from the training set\n",
    "    examples[\"document\"] = [\n",
    "        d.strip() for d in examples[\"text\"] if len(d) > 0 and not d.startswith(\" =\")\n",
    "    ]\n",
    "    # Split the documents from the dataset into it's individual sentences\n",
    "    examples[\"sentences\"] = [\n",
    "        nltk.tokenize.sent_tokenize(document) for document in examples[\"document\"]\n",
    "    ]\n",
    "    # Convert the tokens into ids using the trained tokenizer\n",
    "    examples[\"tokenized_sentences\"] = [\n",
    "        [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent)) for sent in doc]\n",
    "        for doc in examples[\"sentences\"]\n",
    "    ]\n",
    "\n",
    "    # Define the outputs\n",
    "    examples[\"input_ids\"] = []\n",
    "    examples[\"token_type_ids\"] = []\n",
    "    examples[\"attention_mask\"] = []\n",
    "    examples[\"next_sentence_label\"] = []\n",
    "\n",
    "    for doc_index, document in enumerate(examples[\"tokenized_sentences\"]):\n",
    "\n",
    "        current_chunk = []  # a buffer stored current working segments\n",
    "        current_length = 0\n",
    "        i = 0\n",
    "        target_seq_length = max_num_tokens\n",
    "\n",
    "        if random.random() < SHORT_SEQ_PROB:\n",
    "            target_seq_length = random.randint(2, max_num_tokens)\n",
    "\n",
    "        while i < len(document):\n",
    "            segment = document[i]\n",
    "            current_chunk.append(segment)\n",
    "            current_length += len(segment)\n",
    "            if i == len(document) - 1 or current_length >= target_seq_length:\n",
    "                if current_chunk:\n",
    "                    a_end = 1\n",
    "                    if len(current_chunk) >= 2:\n",
    "                        a_end = random.randint(1, len(current_chunk) - 1)\n",
    "\n",
    "                    tokens_a = []\n",
    "                    for j in range(a_end):\n",
    "                        tokens_a.extend(current_chunk[j])\n",
    "\n",
    "                    tokens_b = []\n",
    "\n",
    "                    if len(current_chunk) == 1 or random.random() < NSP_PROB:\n",
    "                        is_random_next = True\n",
    "                        target_b_length = target_seq_length - len(tokens_a)\n",
    "\n",
    "                        for _ in range(10):\n",
    "                            random_document_index = random.randint(\n",
    "                                0, len(examples[\"tokenized_sentences\"]) - 1\n",
    "                            )\n",
    "                            if random_document_index != doc_index:\n",
    "                                break\n",
    "\n",
    "                        random_document = examples[\"tokenized_sentences\"][\n",
    "                            random_document_index\n",
    "                        ]\n",
    "                        random_start = random.randint(0, len(random_document) - 1)\n",
    "                        for j in range(random_start, len(random_document)):\n",
    "                            tokens_b.extend(random_document[j])\n",
    "                            if len(tokens_b) >= target_b_length:\n",
    "                                break\n",
    "                        num_unused_segments = len(current_chunk) - a_end\n",
    "                        i -= num_unused_segments\n",
    "                    else:\n",
    "                        is_random_next = False\n",
    "                        for j in range(a_end, len(current_chunk)):\n",
    "                            tokens_b.extend(current_chunk[j])\n",
    "\n",
    "                    input_ids = tokenizer.build_inputs_with_special_tokens(\n",
    "                        tokens_a, tokens_b\n",
    "                    )\n",
    "                    # add token type ids, 0 for sentence a, 1 for sentence b\n",
    "                    token_type_ids = tokenizer.create_token_type_ids_from_sequences(\n",
    "                        tokens_a, tokens_b\n",
    "                    )\n",
    "\n",
    "                    padded = tokenizer.pad(\n",
    "                        {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids},\n",
    "                        padding=\"max_length\",\n",
    "                        max_length=MAX_LENGTH,\n",
    "                    )\n",
    "\n",
    "                    examples[\"input_ids\"].append(padded[\"input_ids\"])\n",
    "                    examples[\"token_type_ids\"].append(padded[\"token_type_ids\"])\n",
    "                    examples[\"attention_mask\"].append(padded[\"attention_mask\"])\n",
    "                    examples[\"next_sentence_label\"].append(1 if is_random_next else 0)\n",
    "                    current_chunk = []\n",
    "                    current_length = 0\n",
    "            i += 1\n",
    "\n",
    "    # We delete all the un-necessary columns from our dataset\n",
    "    del examples[\"document\"]\n",
    "    del examples[\"sentences\"]\n",
    "    del examples[\"text\"]\n",
    "    del examples[\"tokenized_sentences\"]\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    prepare_train_features, batched=True, remove_columns=[\"text\"], num_proc=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9136e2db-1a36-4337-8a42-f13d377f00f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "collater = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=MLM_PROB, return_tensors=\"tf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "244069ce-ce97-49d6-9311-eb5577435be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tokenized_dataset[\"train\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"],\n",
    "    label_cols=[\"labels\", \"next_sentence_label\"],\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collater,\n",
    ")\n",
    "\n",
    "validation = tokenized_dataset[\"validation\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"],\n",
    "    label_cols=[\"labels\", \"next_sentence_label\"],\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collater,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7f309d1-a856-402f-aef5-ba44dee3ceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "config = BertConfig.from_pretrained(MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61fb1962-cded-44cb-a879-038d02853666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForPreTraining\n",
    "model = TFBertForPreTraining(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e94dce5-a384-4058-8e1b-c7bdd32a8d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d52bec54-93a7-4c9a-9ba8-207d3314035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6dc19e-a5e6-4ca1-99e9-cd265e6b3cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function infer_framework at 0x00000266980E2FC0> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "481/481 [==============================] - ETA: 0s - loss: 8.6761"
     ]
    }
   ],
   "source": [
    "model.fit(train, validation_data=validation, epochs=MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bafe6e2-de20-4683-adb8-5aba2910ae9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
